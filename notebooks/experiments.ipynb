{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "J'ai tout d'abord commencé par uploader le dataset sur [Hugging Face](https://huggingface.co/datasets/Alanox/stanford-dogs) à la fois pour apprendre à utiliser l'upload de dataset mais également pour faire partager ce dataset facilement à la communauté.\n",
        "\n",
        "Testons que cela fonctione bien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\"Alanox/stanford-dogs\", split=\"full\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset[0][\"image\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "L'avantage est que le dataset entier n'est pas chargé ! On charge uniquement ce dont on a besoin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Données\n",
        "\n",
        "Regardons un peu les données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.style.use(['ggplot', 'https://raw.githubusercontent.com/AlanBlanchet/matplotlib_styles/master/vscode_blue.mplstyle'])\n",
        "\n",
        "def add_shape(batch):\n",
        "    batch[\"size\"] = batch[\"image\"].size\n",
        "    return batch\n",
        "\n",
        "ds = dataset.map(add_shape).select_columns([\"name\", \"target\", \"annotations\", \"size\"])\n",
        "\n",
        "df = pd.DataFrame(ds.to_dict())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(24,9))\n",
        "counts = df[\"target\"].value_counts()\n",
        "\n",
        "plt.title(\"Target distribution\")\n",
        "plt.bar(counts.index, counts)\n",
        "plt.xticks(ha=\"right\", rotation=45);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_annots = pd.DataFrame(df[\"annotations\"].explode().reset_index(drop=True).tolist(), columns=[\"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
        "df_annots.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_annots.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(16,9), sharex=True, sharey=True)\n",
        "axs:list[plt.Axes] = np.array(axs).flatten()\n",
        "\n",
        "fig.suptitle(\"Coordinate distributions\")\n",
        "for ax, (name, coords) in zip(axs, df_annots.T.iterrows()):\n",
        "    sorted_coords = coords.sort_values()\n",
        "    ax.set_title(name)\n",
        "    ax.plot(sorted_coords.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pixel_area(box:pd.DataFrame):\n",
        "    box_annots = box[\"annotations\"].explode()\n",
        "    box_area = box_annots.apply(lambda r: (r[2] - r[0]) * (r[3] - r[1]))\n",
        "    return box_area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_areas = df.groupby(\"target\").apply(pixel_area)\n",
        "target_mean_areas = target_areas.groupby(\"target\").apply(np.mean)\n",
        "target_mean_areas_full = df.groupby(\"target\").apply(lambda x: np.mean(x[\"size\"].apply(lambda y: y[0] * y[1])))\n",
        "target_mean_areas.head(), target_mean_areas_full.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_mean_areas = target_mean_areas.sort_values()\n",
        "\n",
        "plt.figure(figsize=(24,9))\n",
        "\n",
        "plt.title(\"Mean annotation box area per target\")\n",
        "plt.bar(target_mean_areas.index, target_mean_areas)\n",
        "plt.ylabel(\"pixel**2\")\n",
        "plt.xticks(ha=\"right\", rotation=45);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On remarque qu'il y a plus de pixels représentant un \"Irish Water Spaniel\" qu'un \"English Foxhound\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "first_targets = df.drop_duplicates([\"target\"], keep=\"first\").reset_index().set_index(\"target\")\n",
        "first_targets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "idx_english_foxhound = int(first_targets.loc[\"English Foxhound\"][\"index\"])\n",
        "idx_irish_water_spaniel = int(first_targets.loc[\"Irish Water Spaniel\"][\"index\"])\n",
        "idx_english_foxhound, idx_irish_water_spaniel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = dataset[idx_english_foxhound][\"image\"]\n",
        "print(img.size)\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = dataset[idx_irish_water_spaniel][\"image\"]\n",
        "print(img.size)\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalisons nos résultats pour avoir un ratio par rapport aux images. Certains chiens peuvent prendre beaucoup d'espace sur une image tandis que d'autres en prennent peut être moins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_mean_areas_ratio = (target_mean_areas / target_mean_areas_full).sort_values()\n",
        "\n",
        "plt.figure(figsize=(24,9))\n",
        "\n",
        "plt.title(\"Mean annotation box area per target normalized\")\n",
        "plt.bar(target_mean_areas_ratio.index, target_mean_areas_ratio)\n",
        "plt.ylabel(\"Mean pixel ratio\")\n",
        "plt.xticks(ha=\"right\", rotation=45);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "first_targets = df.drop_duplicates([\"target\"], keep=\"first\").reset_index().set_index(\"target\")\n",
        "\n",
        "idx_chesapeake_bay_retriever = int(first_targets.loc[\"Chesapeake Bay Retriever\"][\"index\"])\n",
        "idx_irish_water_spaniel = int(first_targets.loc[\"Irish Water Spaniel\"][\"index\"])\n",
        "idx_chesapeake_bay_retriever, idx_irish_water_spaniel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = dataset[idx_chesapeake_bay_retriever][\"image\"]\n",
        "img.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.transforms.v2 as T\n",
        "\n",
        "torchvision.disable_beta_transforms_warning()\n",
        "\n",
        "dog = dataset.with_format(\"pytorch\")[0]\n",
        "img = dog[\"image\"]\n",
        "img_name = dog[\"name\"]\n",
        "transforms = T.Compose([\n",
        "    lambda x: x.permute(2, 0, 1),\n",
        "    T.Resize(400, antialias=True)\n",
        "])\n",
        "F.to_pil_image(transforms(img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "applies = [\n",
        "    T.AugMix(),\n",
        "    T.AutoAugment(),\n",
        "    T.CenterCrop(200),\n",
        "    T.ColorJitter(),\n",
        "    T.ElasticTransform(50.0, 1.0),\n",
        "    T.Grayscale(),\n",
        "    T.GaussianBlur(5),\n",
        "    T.Pad(30),\n",
        "    T.RandomAdjustSharpness(2, p=1),\n",
        "    T.RandomAutocontrast(p=1),\n",
        "    T.RandomCrop(200, 200),\n",
        "    T.RandomHorizontalFlip(p=1),\n",
        "    T.RandomVerticalFlip(p=1),\n",
        "    T.RandomInvert(p=1),\n",
        "    T.RandomPerspective(p=1),\n",
        "    T.RandomPhotometricDistort(p=1),\n",
        "    T.RandomPosterize(4, p=1),\n",
        "    T.RandomZoomOut(p=1),\n",
        "    T.RandomSolarize(0.5, p=1),\n",
        "]\n",
        "\n",
        "n = len(applies)\n",
        "\n",
        "cols = 4\n",
        "rows = -(-n // 4)  # ceil\n",
        "\n",
        "fig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(24, 26))\n",
        "for ax, transform in zip(axs.flatten(), applies):\n",
        "    ax.set_title(type(transform).__name__)\n",
        "    ax.grid(False)\n",
        "    ax.imshow(transform(F.to_pil_image(transforms(img))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bounding box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "masks = dog[\"annotations\"]\n",
        "masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Border"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_annot = img.numpy().copy()\n",
        "\n",
        "for mask in masks:\n",
        "    x, y, w, h = mask.numpy()\n",
        "\n",
        "    res = img.shape[0] / img.shape[1]\n",
        "\n",
        "    img_annot = cv2.rectangle(img_annot, (x,y), (x+w, y+h), (0,0,255), thickness=max(int(res*3), 1))\n",
        "\n",
        "F.to_pil_image(img_annot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_annot = img.numpy().copy()\n",
        "\n",
        "for mask in masks:\n",
        "    x, y, w, h = mask.numpy()\n",
        "\n",
        "    res = img.shape[0] / img.shape[1]\n",
        "\n",
        "    zeros = np.zeros([*img_annot.shape[:-1], 3], dtype=np.uint8)\n",
        "    zeros = cv2.rectangle(zeros, (x,y), (x+w, y+h), (0,0,255), thickness=-1)\n",
        "\n",
        "    img_annot = cv2.addWeighted(img_annot, 1, zeros, 1, 0)\n",
        "\n",
        "print(img_annot.shape)\n",
        "F.to_pil_image(img_annot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Normalisation\n",
        "\n",
        "Some images don't have a normalized luminosity or contrast. The model could be biased by the correlated data from each neighbor pixel.\n",
        "\n",
        "A remedy for this is [PCA/ZCA Whitening](https://www-cs.stanford.edu/~acoates/papers/coatesng_nntot2012.pdf) that I will implement on an example for a dog image.\n",
        "\n",
        "[Here](https://github.com/hadrienj/Preprocessing-for-deep-learning) is also a really good explanation of how ZCA works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
        "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")\n",
        "\n",
        "def square_return_pt_PIL_from_PIL(img):\n",
        "    size = min([*img.size])\n",
        "    img = img.resize((size,size))\n",
        "    return img, F.pil_to_tensor(img).float()\n",
        "\n",
        "\n",
        "def plot_channels(row):\n",
        "    img, img_t = square_return_pt_PIL_from_PIL(row[\"image\"])\n",
        "\n",
        "    # Put channel as first dimension\n",
        "    img_data:torch.Tensor = img_t.permute(2, 0, 1)\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(21,7), sharey=True)\n",
        "    axs:list[plt.Axes] = axs.flatten()\n",
        "\n",
        "    fig.suptitle(f\"Channels intensities count of {row['name']} {img.size}\")\n",
        "    # Color itensities\n",
        "    for chan, c, ax in zip(img_data, img.mode, axs):\n",
        "\n",
        "        intensities = chan.flatten()\n",
        "\n",
        "        ax.set_title(c)\n",
        "\n",
        "        sns.histplot(intensities, ax=ax, color=c.lower(), bins=range(255))\n",
        "\n",
        "plot_channels(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to find the whitening per-pixel, thus we need to iterate threw all the images.\n",
        "\n",
        "The images all need to be of same size. We will save the values of the transformation for the specific size to apply it during training.\n",
        "\n",
        "We will be using a sample of the data since our dataset is large.\n",
        "\n",
        "First we resize the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def resize(size, col_name):\n",
        "    def resize_col(batch):\n",
        "        batch[col_name] = [i.resize((size, size)) for i in batch[\"image\"]]\n",
        "        return batch\n",
        "    \n",
        "    return resize_col\n",
        "\n",
        "size = 32\n",
        "col_size = f\"s{size}\"\n",
        "# Keep images as arrow (on disk)\n",
        "dataset = dataset.map(resize(size, col_size), batched=True, num_proc=8)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the mean per-pixel, covariance matrix and calculate the SVD.\n",
        "\n",
        "SVD for high covariance matrix shapes is heavy in memory and calculations. We will only do whitening for images lower than 32x32x3 and skip the heavy computation steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "rand_idx = np.random.choice(range(len(dataset)), size=5000)\n",
        "subset = dataset.select(rand_idx)\n",
        "\n",
        "imgs = subset.with_format(\"pt\")[col_size] / 255\n",
        "imgs_f = imgs.flatten(start_dim=1)\n",
        "imgs_mean = imgs_f.mean(axis=0)\n",
        "print(\"Mean/Cov...\")\n",
        "imgs_f -= imgs_mean\n",
        "imgs_cov = imgs_f.T.cov()\n",
        "# Vector decomposition\n",
        "print(\"SVD...\")\n",
        "U,S,V = torch.svd(imgs_cov)\n",
        "e = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "e = 0.1\n",
        "\n",
        "ZCA_mat = (U @ np.diag(1.0 / np.sqrt(S + e))) @ U.T\n",
        "print(\"ZCA shape = \", ZCA_mat.shape)\n",
        "\n",
        "def zca(imgs:np.ndarray):\n",
        "    shape = imgs.shape\n",
        "    imgs = imgs.flatten(start_dim=1)\n",
        "    imgs_zca = (ZCA_mat @ imgs.T).T\n",
        "    imgs_zca = (imgs_zca - imgs_zca.min()) / (imgs_zca.max() - imgs_zca.min())\n",
        "    return imgs_zca.view(shape)\n",
        "\n",
        "imgs_zca = zca(imgs)\n",
        "print(imgs_zca.shape)\n",
        "imgs_zca = imgs_zca.permute(0, 3, 1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_shown = 8\n",
        "num_offset = 8\n",
        "\n",
        "plt.tight_layout()\n",
        "fig, axs = plt.subplots(2, num_shown, figsize=(16,4))\n",
        "\n",
        "for col_axs, imgs_axs in zip(axs, ([F.to_pil_image(i).resize((size, size)) for i in imgs_zca[num_offset:num_offset+num_shown]], subset.select(np.arange(num_offset, num_offset+num_shown))[col_size])):\n",
        "    for ax, img in zip(col_axs, imgs_axs):\n",
        "        ax.imshow(img)\n",
        "        ax.axis(\"off\")\n",
        "        ax.grid(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Supervised\n",
        "\n",
        "Here we randomly chose classes and run UMAP on the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "targets = df[\"target\"].unique()\n",
        "num_classes = 5\n",
        "\n",
        "np.random.seed(7)\n",
        "rand_classes = np.random.choice(range(len(targets)), size=num_classes)\n",
        "select_targets = targets[rand_classes]\n",
        "selected_idx = df.loc[df[\"target\"].isin(select_targets)].index\n",
        "\n",
        "print(\"Classes = \", select_targets)\n",
        "\n",
        "mapped = dataset.select(selected_idx).with_format(\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "flats = mapped[col_size].flatten(start_dim=1)\n",
        "\n",
        "print(flats.shape)\n",
        "scaled = StandardScaler().fit_transform(flats)\n",
        "print(scaled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import umap\n",
        "import seaborn as sns\n",
        "\n",
        "reducer = umap.UMAP(n_epochs=100, random_state=0)\n",
        "dd = reducer.fit_transform(scaled)\n",
        "plt.figure(figsize=(16,9))\n",
        "sns.scatterplot(x=dd[:,0],y=dd[:,1], hue=mapped[\"target\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With ZCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mapped_zca = zca(mapped[col_size].float())\n",
        "\n",
        "scaled = StandardScaler().fit_transform(mapped_zca.flatten(start_dim=1))\n",
        "\n",
        "reducer = umap.UMAP(n_epochs=100, random_state=0)\n",
        "dd = reducer.fit_transform(scaled)\n",
        "plt.figure(figsize=(16,9))\n",
        "sns.scatterplot(x=dd[:,0],y=dd[:,1], hue=mapped[\"target\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "oc5-2XWLAjSi-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
